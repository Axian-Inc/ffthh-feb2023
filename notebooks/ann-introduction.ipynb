{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How Do Machines Learn?\n",
    "\n",
    "In the last talk on machine learning, we trained a random forest classifier to classify digitized, handwritten images of digits into numerical symbols from 0 to 9.\n",
    "\n",
    "However, in that discussion, we glossed over a few important details about how these models 'learn'. \n",
    "\n",
    "## Following patterns found in nature\n",
    "\n",
    "In answering the question about how machines learn, one thing that might be helpful to ask is how people, or other biological systems, learn.\n",
    "\n",
    "A biological neuron accumulates electrical charge on the cell membranes next to the axon. The electrical charges are accumulated by action from the set of multiple presynaptic neurons that provide input to the cell.  \n",
    "\n",
    "_Neurotransmitters_ in the _synaptic cleft_ instruct the cell to open or close _ion channels_ in the neuron.  Ions travelling in or out of the cell change the the membrane potential, until a point at which an _Action potential_ is achieved, which causes electrical charge to be dissipated down the axon and towards the _postsynaptic neuron_, which starts the process anew for downstream neurons.\n",
    "\n",
    "![image](bioneuron.png \"a biological neuron\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuron as Model\n",
    "\n",
    "If we distill the functionality of the function of a neuron into a functional unit, we can see it as a function that has several parts\n",
    "\n",
    "- an input vector representing the set of inputs from presynaptic neurons\n",
    "- a bias term representing the sensitivity of the neuron itself\n",
    "- an activation function\n",
    "\n",
    "![image](modelneuron.png \"a model neuron\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simplified, mathematical represenation of a neuron, used for computing, was called a _Perceptron_, and has been around for a long time (Rosenblatt, 1957)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a mathematical representation of the neuron, we can compute it \n",
    "import numpy as np\n",
    "\n",
    "example_input = [1, .2, .1, .05, .2]\n",
    "\n",
    "# these are arbitrary\n",
    "example_weights = [.2, .12, .4, .6, .90]\n",
    "\n",
    "def perceptron(input, weights, threshold):\n",
    "    \"\"\"\n",
    "    aggregates input, applying the weights, compares the aggregated input against a threshold and producing an output.\n",
    "    \"\"\"\n",
    "    input_vector = np.array(input)\n",
    "    weights = np.array(weights)\n",
    "    bias_weight = .2\n",
    "\n",
    "    # summing the activation uses the dot product\n",
    "    activation_level = np.dot(input_vector, weights) + (bias_weight * 1)                                \n",
    "    activation_level\n",
    "    \n",
    "    if activation_level >= threshold:\n",
    "        perceptron_output = 1\n",
    "    else:\n",
    "        perceptron_output = 0\n",
    "    return perceptron_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "output = perceptron(example_input, example_weights, 0.5)\n",
    "\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But Where's the Learning?\n",
    "\n",
    "So far, this explains a bit about how to represent a neuron mathematically, but it doesn't provide us with a lot of information about how it learns. \n",
    "\n",
    "It learns by changing the weights, based on whether the output of the system is correct or incorrect (so, in this case, it's supervised-learning).\n",
    "\n",
    "As it's supervised learning, that means that we know the answers to the question that the model is trying to solve, beforehand.  Let's add a very basic supervised learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_weights(perceptron_output, \n",
    "    expected_output, \n",
    "    given_input,\n",
    "    input_weights,\n",
    "    learning_rate=1):\n",
    "    \"\"\"\n",
    "    takes observed perceptron output, and input weights, and gives a new set of weights\n",
    "    \"\"\"\n",
    "    new_weights = []\n",
    "    for i, x in enumerate(given_input):\n",
    "\n",
    "        new_weight = input_weights[i] + (expected_output - perceptron_output) * x * learning_rate\n",
    "        new_weights.append(new_weight)\n",
    "    return np.array(new_weights)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the function has a \"learning rate\" parameter.  We'll come back to that in a little while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i is 0\n",
      "x is 1\n",
      "new_weight is -0.8\n",
      "i is 1\n",
      "x is 0.2\n",
      "new_weight is -0.08000000000000002\n",
      "i is 2\n",
      "x is 0.1\n",
      "new_weight is 0.30000000000000004\n",
      "i is 3\n",
      "x is 0.05\n",
      "new_weight is 0.5499999999999999\n",
      "i is 4\n",
      "x is 0.2\n",
      "new_weight is 0.7\n",
      "old weights are: [0.2, 0.12, 0.4, 0.6, 0.9]\n",
      "new weights are [-0.8  -0.08  0.3   0.55  0.7 ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "expected_output = 0\n",
    "new_weights = adjust_weights(output, 0, example_input, example_weights)\n",
    "\n",
    "print(f'old weights are: {example_weights}')\n",
    "print(f'new weights are {new_weights}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output before learning is 1\n",
      "output_after_learning is 0\n"
     ]
    }
   ],
   "source": [
    "# Now let's put our learning algorithm and our perceptron together:\n",
    "# Remember the correct output for the the algorithm is 0\n",
    "\n",
    "output = perceptron(example_input, example_weights, 0.5)\n",
    "print(f'output before learning is {output}')\n",
    "new_weights = adjust_weights(output, 0, example_input, example_weights)\n",
    "\n",
    "output_again = perceptron(example_input, new_weights, 0.5)\n",
    "print(f'output_after_learning is {output_again}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's try to use perceptrons to do a very simple classifier task\n",
    "\n",
    "We can use a perceptron with 2 inputs and one output to try and solve a problem that trains a network to \"understand\" logical OR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-13-ce9685e9bcb7>, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-13-ce9685e9bcb7>\"\u001b[1;36m, line \u001b[1;32m8\u001b[0m\n\u001b[1;33m    expected_results = [0,  # (False OR False) gives False\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# example problem for learning logical OR\n",
    "\n",
    "sample_data = [[0, 0],  # False, False\n",
    "               [0, 1],  # False, True\n",
    "               [1, 0],  # True, False\n",
    "               [1, 1]]  # True, True\n",
    "\n",
    "expected_results = [0,  # (False OR False) gives False\n",
    "                    1,  # (False OR True ) gives True\n",
    "                    1,  # (True  OR False) gives True\n",
    "                    1]  # (True  OR True ) gives True\n",
    "\n",
    "activation_threshold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00074888, 0.00065487])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import random\n",
    "import numpy as np\n",
    "\n",
    "weights = np.random.random(2)/1000  # Small random float 0 < w < .001\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0004419447072414342"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_weight = np.random.random() / 1000\n",
    "bias_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.00041949, 1.00055609])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = []\n",
    "for datum in sample_data:\n",
    "    output.append(perceptron(datum, weights, activation_threshold))\n",
    "\n",
    "output_arr = np.array(output)\n",
    "for idx,datum  in enumerate(sample_data):\n",
    "    new_weights = adjust_weights(output[idx], expected_results[idx], datum, weights)\n",
    "\n",
    "new_weights\n",
    "\n",
    "# This represents one iteration, or \"batch\", of having trained our very simple nerual network.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a function that represents this training.\n",
    "def train_perceptron(input_features, \n",
    "    input_labels, \n",
    "    initial_weights,\n",
    "    activation_threshold,\n",
    "    num_batches, \n",
    "    learning_rate=1):\n",
    "    \"\"\"\n",
    "    Trains a perceptron with given input\n",
    "    \"\"\"\n",
    "\n",
    "    weights = initial_weights\n",
    "    for batch_num in range(0,num_batches):\n",
    "        print(f'pre-batch weights are: {weights}')\n",
    "        output = []\n",
    "        for datum in input_features:\n",
    "            output.append(perceptron(datum, weights, activation_threshold))\n",
    "\n",
    "        for idx,datum  in enumerate(input_features):\n",
    "            weights = adjust_weights(output[idx], input_labels[idx], datum, weights,learning_rate)\n",
    "\n",
    "        print(f'post-batch weights are: {weights}, output is: {output}, expected is {input_labels}')        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-batch weights are: [0.00074888 0.00065487]\n",
      "post-batch weights are: [2.00074888 2.00065487], output is: [0, 0, 0, 0], expected is [0, 1, 1, 1]\n",
      "pre-batch weights are: [2.00074888 2.00065487]\n",
      "post-batch weights are: [2.00074888 2.00065487], output is: [0, 1, 1, 1], expected is [0, 1, 1, 1]\n",
      "pre-batch weights are: [2.00074888 2.00065487]\n",
      "post-batch weights are: [2.00074888 2.00065487], output is: [0, 1, 1, 1], expected is [0, 1, 1, 1]\n",
      "pre-batch weights are: [2.00074888 2.00065487]\n",
      "post-batch weights are: [2.00074888 2.00065487], output is: [0, 1, 1, 1], expected is [0, 1, 1, 1]\n",
      "pre-batch weights are: [2.00074888 2.00065487]\n",
      "post-batch weights are: [2.00074888 2.00065487], output is: [0, 1, 1, 1], expected is [0, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Let's train our simple model with a few batches\n",
    "\n",
    "train_perceptron(sample_data, expected_results, weights, 0.5, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that the simple \"network\" (which was really just one neuron) arrived at a state where it was not \"learning\" at all any more, because the network eventually got all the answers correct.\n",
    "\n",
    "In this case, our toy network converged after only one batch! Its error rate dropped to zero.\n",
    "\n",
    "When a model arrives at a point, after which it does not learn any more with the given training data set, its weights do not change. We call this _convergence_. \n",
    "\n",
    "What if we had tried a learning rate that was not the default (1)? Presumably, this would affect how many iterations it takes to reach convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-batch weights are: [0.00074888 0.00065487]\n",
      "post-batch weights are: [0.10074888 0.10065487], output is: [0, 0, 0, 0], expected is [0, 1, 1, 1]\n",
      "pre-batch weights are: [0.10074888 0.10065487]\n",
      "post-batch weights are: [0.20074888 0.20065487], output is: [0, 0, 0, 0], expected is [0, 1, 1, 1]\n",
      "pre-batch weights are: [0.20074888 0.20065487]\n",
      "post-batch weights are: [0.25074888 0.25065487], output is: [0, 0, 0, 1], expected is [0, 1, 1, 1]\n",
      "pre-batch weights are: [0.25074888 0.25065487]\n",
      "post-batch weights are: [0.30074888 0.30065487], output is: [0, 0, 0, 1], expected is [0, 1, 1, 1]\n",
      "pre-batch weights are: [0.30074888 0.30065487]\n",
      "post-batch weights are: [0.30074888 0.30065487], output is: [0, 1, 1, 1], expected is [0, 1, 1, 1]\n",
      "pre-batch weights are: [0.30074888 0.30065487]\n",
      "post-batch weights are: [0.30074888 0.30065487], output is: [0, 1, 1, 1], expected is [0, 1, 1, 1]\n",
      "pre-batch weights are: [0.30074888 0.30065487]\n",
      "post-batch weights are: [0.30074888 0.30065487], output is: [0, 1, 1, 1], expected is [0, 1, 1, 1]\n",
      "pre-batch weights are: [0.30074888 0.30065487]\n",
      "post-batch weights are: [0.30074888 0.30065487], output is: [0, 1, 1, 1], expected is [0, 1, 1, 1]\n",
      "pre-batch weights are: [0.30074888 0.30065487]\n",
      "post-batch weights are: [0.30074888 0.30065487], output is: [0, 1, 1, 1], expected is [0, 1, 1, 1]\n",
      "pre-batch weights are: [0.30074888 0.30065487]\n",
      "post-batch weights are: [0.30074888 0.30065487], output is: [0, 1, 1, 1], expected is [0, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "train_perceptron(sample_data, expected_results, weights, 0.5, 10, learning_rate=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, adjusting the learning rate to be lower than 1 makes the network converge more slowly.\n",
    "\n",
    "But why would you want a network to converge more slowly? Let's remember this and the other parameters that we've created for the `train_perceptron` function:  learning rate, number of batches, etc, are standard _hyperparameters_ that apply to most neural network models.  Tweaking these hyperparameters has an effect on model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recapping: What did we just do?\n",
    "\n",
    "__TODO put visualization of our toy network here__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some problems with Simple Perceptrons (simple neurons with stepwise or linear activation functions)\n",
    "TODO do i really need this detail\n",
    "\n",
    "The basic perceptron as we implemented it above is very old. In the decades between 1957 and now, there has been a lot of evolution in neural networks, but a few of the big, early problems with neural networks were as follows:\n",
    "\n",
    "1. Perceptrons can't solve problems whose answers aren't linearly separable - that is, more complicated relationships in the patterns of data, that aren't easily delineated by a line on a chart, aren't easily caught.\n",
    "\n",
    "2. There's no simple way to adjust the weights during learning for more than one layer of them\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-batch weights are: [0.02, 0.34]\n",
      "post-batch weights are: [[0.02]\n",
      " [0.29]], output is: [0, 1, 0, 1], expected is [[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n",
      "pre-batch weights are: [[0.02]\n",
      " [0.29]]\n",
      "post-batch weights are: [[0.02]\n",
      " [0.29]], output is: [0, 0, 0, 1], expected is [[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n",
      "pre-batch weights are: [[0.02]\n",
      " [0.29]]\n",
      "post-batch weights are: [[0.02]\n",
      " [0.29]], output is: [0, 0, 0, 1], expected is [[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n",
      "pre-batch weights are: [[0.02]\n",
      " [0.29]]\n",
      "post-batch weights are: [[0.02]\n",
      " [0.29]], output is: [0, 0, 0, 1], expected is [[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n",
      "pre-batch weights are: [[0.02]\n",
      " [0.29]]\n",
      "post-batch weights are: [[0.02]\n",
      " [0.29]], output is: [0, 0, 0, 1], expected is [[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n",
      "pre-batch weights are: [[0.02]\n",
      " [0.29]]\n",
      "post-batch weights are: [[0.02]\n",
      " [0.29]], output is: [0, 0, 0, 1], expected is [[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n",
      "pre-batch weights are: [[0.02]\n",
      " [0.29]]\n",
      "post-batch weights are: [[0.02]\n",
      " [0.29]], output is: [0, 0, 0, 1], expected is [[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n",
      "pre-batch weights are: [[0.02]\n",
      " [0.29]]\n",
      "post-batch weights are: [[0.02]\n",
      " [0.29]], output is: [0, 0, 0, 1], expected is [[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n",
      "pre-batch weights are: [[0.02]\n",
      " [0.29]]\n",
      "post-batch weights are: [[0.02]\n",
      " [0.29]], output is: [0, 0, 0, 1], expected is [[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n",
      "pre-batch weights are: [[0.02]\n",
      " [0.29]]\n",
      "post-batch weights are: [[0.02]\n",
      " [0.29]], output is: [0, 0, 0, 1], expected is [[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "# example of an \"XOR\" problem, which does NOT have a linearly separable solution:\n",
    "\n",
    "xor_features = np.array([[0, 0],\n",
    "                    [0, 1],\n",
    "                    [1, 0],\n",
    "                    [1, 1]])\n",
    "xor_labels = np.array([[0],\n",
    "                    [1],\n",
    "                    [1],\n",
    "                    [0]])\n",
    "\n",
    "\n",
    "\n",
    "train_perceptron(xor_features, xor_labels, [0.02, 0.34], 0.5, 10, learning_rate=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Solutions to these problems:\n",
    "1. Changing the activation function\n",
    "    \n",
    "2. Backpropagation algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are modern neural network APIs like? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 10)                30        \n",
      "                                                                 \n",
      " activation (Activation)     (None, 10)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 41\n",
      "Trainable params: 41\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential                 \n",
    "from keras.layers import Dense, Activation          \n",
    "from keras.optimizers import SGD      \n",
    "\n",
    "\n",
    "# Our examples for an exclusive OR.\n",
    "x_train = np.array([[0, 0],\n",
    "                    [0, 1],\n",
    "                    [1, 0],\n",
    "                    [1, 1]])                        \n",
    "y_train = np.array([[0],\n",
    "                    [1],\n",
    "                    [1],\n",
    "                    [0]])       \n",
    "\n",
    "model = Sequential()\n",
    "num_neurons = 10\n",
    "model.add(Dense(num_neurons, input_dim=2))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTES:\n",
    "\n",
    "The notes on loss functions and gradient descent, they come from 3-blue-1-brown's great video on the topic\n",
    "\n",
    "Several of the images and code examples in this notebook come from, or are adapted from, Hobson Lane et al.'s\\ text: \n",
    "\n",
    "[Natural Language Processing in Action](https://www.manning.com/books/natural-language-processing-in-action)\n",
    "\n",
    "If you are interested in NLP and machine learning in general, this is a great book.  It was written by members of Portland's own local Data Science Meetup group.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "912e0425320a54aa72daae9181d14c508cccc24d8168b9331594aaceab410339"
  },
  "kernelspec": {
   "display_name": "Python 3.8.16 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
